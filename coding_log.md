# todo list
1. ~~每个batch 是否需要detach？ 先使用detach试试效果~~
2. ~~conv和lstm是否使用bias？ 先使用bias试试。~~

~~batch norm的参数不一致， 已修改为一致。~~

~~batch first的效果更好。~~

~~损失的计算方式问题？ 没问题，是网络的输出值不一样。~~

~~将所有的优化参数放到一个adam中，测试中。。。 没有变得更好。~~

~~梯度裁剪问题？ 没问题~~

~~优化器问题？~~

​	~~是否tf中，rnn的最后一层没有自带dropout？ 需要带~~

torch版本的效果并不好，查找原因：

网络结构问题？



batch norm 的参数不一样，测试中，





泛化的稳定性还是不够好





batch norm 等于0.01的版本取得了较好的效果，而且比较稳定。







目前较好的版本是：不使用正则化，batch norm 等于0.01



两个版本在测试：

在cnn上加l2正则化

在整个网络上加l2正则化



最后是不使用L2正则化的版本最好。虽然最后结果差不多。

最后还是使用cnn正则化的版本。

