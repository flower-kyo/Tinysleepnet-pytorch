# todo list
1. ~~每个batch 是否需要detach？ 先使用detach试试效果~~
2. ~~conv和lstm是否使用bias？ 先使用bias试试。~~

~~batch norm的参数不一致， 已修改为一致。~~

~~batch first的效果更好。~~

~~损失的计算方式问题？ 没问题，是网络的输出值不一样。~~

~~将所有的优化参数放到一个adam中，测试中。。。 没有变得更好。~~

~~梯度裁剪问题？ 没问题~~

~~优化器问题？~~

​	~~是否tf中，rnn的最后一层没有自带dropout？ 需要带~~

torch版本的效果并不好，查找原因：

网络结构问题？batch norm 的参数不一样，泛化的稳定性还是不够好。batch norm 等于0.01的版本取得了较好的效果，而且比较稳定。目前较好的版本是：不使用正则化，batch norm 等于0.01

最后是不使用L2正则化的版本最好。虽然最后结果差不多。最后还是使用cnn正则化的版本，保持与原文一致。

尝试对数据做简单的标准化，以及分解波形之后的标准化。

接下来完善评价指标和保存加载模型的代码。

- [x] 完善评价指标
- [x] 保存模型
- [x] 编写predict文件
- [x] 原来的代码在测试的时候没有固定BN层的参数。设置BN层在测试时为固定状态，测试中。当测试样本没有参与BN训练时，效果会表差。这更加说明个体之间的差异性。
- [ ] 针对个体差异定制的神经网络Norm
- [ ] NCE-Norm？
- [ ] 学习集中norm的方式，提出针对个体的norm
- [ ] tinysleepnet的训练方式：在每个epoch的最后几次训练中，全是W期的数据，而且每个batch中，有效的样本数不足300，导致最后网络优化的方向偏向于W。
- [ ] 改用SGD优化器，调整学习率





