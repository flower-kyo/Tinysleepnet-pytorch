# todo list
1. ~~每个batch 是否需要detach？ 先使用detach试试效果~~
2. ~~conv和lstm是否使用bias？ 先使用bias试试。~~

~~batch norm的参数不一致， 已修改为一致。~~

~~batch first的效果更好。~~

~~损失的计算方式问题？ 没问题，是网络的输出值不一样。~~

~~将所有的优化参数放到一个adam中，测试中。。。 没有变得更好。~~

~~梯度裁剪问题？ 没问题~~

~~优化器问题？~~

​	~~是否tf中，rnn的最后一层没有自带dropout？ 需要带~~

torch版本的效果并不好，查找原因：

网络结构问题？



batch norm 的参数不一样，测试中，





泛化的稳定性还是不够好





batch norm 等于0.01的版本取得了较好的效果，而且比较稳定。









